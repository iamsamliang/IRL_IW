{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msdm.domains import GridWorld\n",
    "from msdm.algorithms.entregpolicyiteration import entropy_regularized_policy_iteration\n",
    "from msdm.core.problemclasses.mdp import TabularPolicy\n",
    "from msdm.core.distributions import DictDistribution\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from frozendict import frozendict\n",
    "from dataset import TrajectoryDataset, FeaturesDataset\n",
    "from algorithms import MaxLikelihoodIRL, ImitationLearning\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_trajs = 10000\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "lr = 1\n",
    "weight_decay = 0\n",
    "momentum = 0.9\n",
    "entropy_weight = 2\n",
    "discount_rate = 0.99\n",
    "step_cost = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<msdm.domains.gridworld.plotting.GridWorldPlotter at 0x190d20ec9d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2664x792 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a gridworld\n",
    "mdp = GridWorld(\n",
    "    tile_array=[\n",
    "        \"sb....c.c..c.bbx...xx.xx.xxx.bx..bc.g\",\n",
    "        \".axxxx.xxx.x.b.c.x..aaaa....xxx...x..\",\n",
    "        \"c...x....xbac..ax..xbx.....bbb....xb.\",\n",
    "        \"x.x...xx.x....c..ab.....xcx....x..x.a\",\n",
    "        \"..xxaa....c...b..ba..xx....ac..b..a.x\",\n",
    "        \"..xx..xbaa..caxx........abbc..x...x..\",\n",
    "        \".x.bb.x..x.xxcxx..ba..xx..axx...ab..c\",\n",
    "        \".x.b.ax..c.bxbxc.xx...xxxx..x..c..b..\",\n",
    "        \"...bba.x.x.xxb........xxxa.bb...xbb..\",\n",
    "        \".x...a...a..ax.x.bxx..xa......x..aaa.\",\n",
    "        \"sx..bax..x.x...c...ab....xxx.ab..cbcb\",\n",
    "    ],\n",
    "    feature_rewards={\n",
    "        '.': 0,\n",
    "        'a': 0,\n",
    "        'g': 10,\n",
    "        'x': -12,\n",
    "        'c': -5,\n",
    "        'b': -1,\n",
    "    },\n",
    "    absorbing_features=('g'),\n",
    "    initial_features=('s'),\n",
    "    discount_rate=discount_rate,\n",
    "    step_cost=step_cost,\n",
    ")\n",
    "featurecolors = {\n",
    "    '.': 'white',\n",
    "    'a': 'lightgreen',\n",
    "    'g': 'green',\n",
    "    'x': 'red',\n",
    "    'c': 'black',\n",
    "    'b': 'blue'\n",
    "}\n",
    "mdp.plot(featurecolors=featurecolors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Maximum Entropy IRL policy to generate trajectories\n",
    "expert_erpi_params = dict(\n",
    "    transition_matrix=torch.tensor(mdp.transition_matrix),\n",
    "    reward_matrix=torch.tensor(mdp.reward_matrix),\n",
    "    discount_rate=torch.tensor(mdp.discount_rate),\n",
    "    # the lower this is, the more optimal the policy\n",
    "    entropy_weight=torch.tensor([entropy_weight]),\n",
    "    n_planning_iters=10,\n",
    "    policy_prior=None,\n",
    "    initial_policy=None,\n",
    "    check_convergence=True,\n",
    "    force_nonzero_probabilities=True,\n",
    ")\n",
    "\n",
    "# Max Entropy IRL expert policy\n",
    "expert_erpi = entropy_regularized_policy_iteration(\n",
    "    **expert_erpi_params\n",
    ")\n",
    "\n",
    "expert_policy = TabularPolicy.from_matrix(\n",
    "    states=mdp.state_list,\n",
    "    actions=mdp.action_list,\n",
    "    policy_matrix=expert_erpi.policy.detach().numpy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our numpy 2d array to a tensor for training\n",
    "# do I need to convert the labels to a tensor? yes, does it for us in Lambda (target_transform)\n",
    "class ToTensor(object):\n",
    "  \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "  def __call__(self, sample):\n",
    "    return torch.from_numpy(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "ActionClassifier(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "class ActionClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ActionClassifier, self).__init__()\n",
    "    self.conv1 = nn.Sequential(\n",
    "        nn.Conv1d(1, 16, 3, padding=1),\n",
    "        nn.BatchNorm1d(16),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.conv2 = nn.Sequential(\n",
    "        nn.Conv1d(16, 32, 3, padding=1),\n",
    "        nn.BatchNorm1d(32),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.conv3 = nn.Sequential(\n",
    "        nn.Conv1d(32, 64, 3, stride=2, padding=1),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 5),\n",
    "    )\n",
    "    # don't use softmax bc nn.CrossEntropyLoss takes unnormalized outputs\n",
    "    # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_conv1 = self.conv1(x)\n",
    "    x_conv2 = self.conv2(x_conv1)\n",
    "    x_conv3 = self.conv3(x_conv2)\n",
    "    x_flat = self.flatten(x_conv3)\n",
    "    logits = self.linear_relu_stack(x_flat)\n",
    "    # probabilities = self.softmax(logits)\n",
    "    return logits\n",
    "\n",
    "\n",
    "model = ActionClassifier().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate gen_iters of trajectories\n",
    "gen_iters = num_trajs\n",
    "trajectories = []\n",
    "\n",
    "for _ in range(gen_iters):\n",
    "  # Starts at one of the labelled starting points\n",
    "  trajectory = expert_policy.run_on(mdp)\n",
    "  formatted_traj = {}\n",
    "  formatted_traj['state_traj'] = trajectory.state_traj\n",
    "  formatted_traj['action_traj'] = trajectory.action_traj\n",
    "  trajectories.append(formatted_traj)\n",
    "\n",
    "trajs_dataset = TrajectoryDataset(trajectories)\n",
    "features_dataset = FeaturesDataset(mdp, trajectories, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cross Entropy Loss for classification\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# better optimizer with a scheduler to decrease learning rate by 0.1 at indicated steps\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.1, weight_decay=0.0001, momentum=momentum)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, [10, 20, 30, 40], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurizer(state, action, next_state):\n",
    "  feature_name = mdp.location_features.get(next_state, 's')\n",
    "  if feature_name in 's':\n",
    "    return {}\n",
    "  return {feature_name: 1}\n",
    "\n",
    "def fixed_reward(state, action, next_state):\n",
    "  return mdp.step_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = MaxLikelihoodIRL(mdp, featurizer, fixed_reward, batch_size=batch_size, epochs=epochs,\n",
    "                             lr=lr, momentum=momentum, entropy_weight=entropy_weight, weight_decay=weight_decay)\n",
    "\n",
    "supervised_learning = ImitationLearning(\n",
    "    mdp, _, _, model, loss_fn, optimizer, scheduler=scheduler, batch_size=batch_size, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Inital reward weights: tensor([-0.5433,  0.6341, -0.0795,  0.3986,  0.2031], dtype=torch.float64)\n",
      "\n",
      "loss: 1.565497  [    0/10000]\n",
      "loss: 1.259949  [    0/10000]\n",
      "loss: 1.238478  [    0/10000]\n",
      "loss: 1.206998  [    0/10000]\n",
      "loss: 1.211028  [    0/10000]\n",
      "loss: 1.181767  [    0/10000]\n",
      "loss: 1.238607  [    0/10000]\n",
      "loss: 1.207103  [    0/10000]\n",
      "loss: 1.257708  [    0/10000]\n",
      "loss: 1.174239  [    0/10000]\n",
      "loss: 1.181832  [    0/10000]\n",
      "loss: 1.218641  [    0/10000]\n",
      "loss: 1.235133  [    0/10000]\n",
      "loss: 1.201145  [    0/10000]\n",
      "loss: 1.215880  [    0/10000]\n",
      "loss: 1.235628  [    0/10000]\n",
      "loss: 1.258081  [    0/10000]\n",
      "loss: 1.225600  [    0/10000]\n",
      "loss: 1.252402  [    0/10000]\n",
      "loss: 1.247119  [    0/10000]\n",
      "loss: 1.203840  [    0/10000]\n",
      "loss: 1.218454  [    0/10000]\n",
      "loss: 1.201370  [    0/10000]\n",
      "loss: 1.223774  [    0/10000]\n",
      "loss: 1.273278  [    0/10000]\n",
      "loss: 1.248371  [    0/10000]\n",
      "loss: 1.242469  [    0/10000]\n",
      "loss: 1.204047  [    0/10000]\n",
      "loss: 1.171160  [    0/10000]\n",
      "loss: 1.238055  [    0/10000]\n",
      "loss: 1.209196  [    0/10000]\n",
      "loss: 1.215685  [    0/10000]\n",
      "loss: 1.207710  [    0/10000]\n",
      "loss: 1.202212  [    0/10000]\n",
      "loss: 1.189239  [    0/10000]\n",
      "loss: 1.147114  [    0/10000]\n",
      "loss: 1.186576  [    0/10000]\n",
      "loss: 1.226350  [    0/10000]\n"
     ]
    }
   ],
   "source": [
    "learned_weights, the_policy, max_losses = algorithm.learn(trajs_dataset)\n",
    "optimized_model, imitation_losses = supervised_learning.learn(features_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve with matrices returned by gridworld object\n",
    "max_policy = TabularPolicy.from_matrix(\n",
    "    states=mdp.state_list,\n",
    "    actions=mdp.action_list,\n",
    "    policy_matrix=the_policy.policy.detach().numpy()\n",
    ")\n",
    "\n",
    "mdp.plot(featurecolors=featurecolors).plot_policy(max_policy)\n",
    "mdp.plot().plot_state_map(max_policy.evaluate_on(mdp).V)\n",
    "\n",
    "mdp.plot(featurecolors=featurecolors).plot_policy(expert_policy)\n",
    "mdp.plot().plot_state_map(expert_policy.evaluate_on(mdp).V)\n",
    "\n",
    "optimized_model.eval()\n",
    "\n",
    "temp_imitpolicy = {}\n",
    "int_to_action = {0: frozendict(dx=0, dy=1), 1: frozendict(\n",
    "    dx=0, dy=-1), 2: frozendict(dx=1, dy=0), 3: frozendict(dx=-1, dy=0), 4: frozendict(dx=0, dy=0)}\n",
    "\n",
    "softmax_fn = nn.Softmax(dim=1)\n",
    "\n",
    "# so pred doesn't have a grad_fn attached to it\n",
    "with torch.no_grad():\n",
    "  for s in mdp.state_list:\n",
    "    state_feature = torch.from_numpy(\n",
    "        np.array([features_dataset.getStateFeature(s)], dtype='f')).to(device)\n",
    "    # pred = model(state_feature)[0]\n",
    "    # pred = supervised_learning.model(state_feature)\n",
    "    pred = optimized_model(state_feature)\n",
    "    probs = softmax_fn(pred)[0]\n",
    "    temp_imitpolicy[s] = {}\n",
    "    # for each action in each state, grab from the \"pred\" result the probability of taking this action in this state and add that to the dictionary policy which is a dictionary with states as keys and a dictionary of \"action: prob of taking this action in this state\" as values\n",
    "    for a_i, prob in enumerate(probs):\n",
    "      temp_imitpolicy[s][int_to_action[a_i]] = prob.item()\n",
    "\n",
    "# to turn policy into an actual policy object that we can call the MSDM functions on\n",
    "imitation_policy = TabularPolicy({s: DictDistribution(ap) for s, ap in temp_imitpolicy.items()})\n",
    "\n",
    "mdp.plot(featurecolors=featurecolors).plot_policy(imitation_policy)\n",
    "mdp.plot().plot_state_map(imitation_policy.evaluate_on(mdp).V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max policy initial value: {max_policy.evaluate_on(mdp).initial_value}\")\n",
    "print(f\"Expert policy initial value: {expert_policy.evaluate_on(mdp).initial_value}\")\n",
    "print(f\"Imitation policy initial Value: {imitation_policy.evaluate_on(mdp).initial_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gridworld\n",
    "mdp2 = GridWorld(\n",
    "    tile_array=[\n",
    "        \"sx....c.b..c.bbx...bc.g\",\n",
    "        \".abxxx.xxx.cb..cax..x..\",\n",
    "        \"c...x....x.ac.xx..b...b\",\n",
    "        \"x.x...xx......xxx...x.a\",\n",
    "        \"..xaa..cx..ba.....ac..x\",\n",
    "        \"..xx..xaab..cab..x..x..\",\n",
    "        \".x.bb.c..xccxc.a..ab..c\",\n",
    "        \".x..cax..a..x..bxc.....\",\n",
    "        \"...xa....x.x..b...bbb..\",\n",
    "        \".x...ax..a..a..xx..aaa.\",\n",
    "        \"sx..bac..xb.xx.ab....cc\",\n",
    "    ],\n",
    "    feature_rewards={\n",
    "        '.': 0,\n",
    "        'a': 0,\n",
    "        'g': 10,\n",
    "        'x': -12,\n",
    "        'c': -5,\n",
    "        'b': -1,\n",
    "    }\n",
    "    absorbing_features=('g'),\n",
    "    initial_features=('s'),\n",
    "    discount_rate=discount_rate,\n",
    "    step_cost=step_cost,\n",
    ")\n",
    "featurecolors = {\n",
    "    '.': 'white',\n",
    "    'a': 'lightgreen',\n",
    "    'g': 'green',\n",
    "    'x': 'red',\n",
    "    'c': 'black',\n",
    "    'b': 'blue'\n",
    "}\n",
    "\n",
    "mdp2.plot(featurecolors=featurecolors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_erpi_params2 = dict(\n",
    "    transition_matrix=torch.tensor(mdp2.transition_matrix),\n",
    "    reward_matrix=torch.tensor(mdp2.reward_matrix),\n",
    "    discount_rate=torch.tensor(mdp2.discount_rate),\n",
    "    # the lower this is, the more optimal the policy\n",
    "    entropy_weight=torch.tensor([entropy_weight]),\n",
    "    n_planning_iters=10,\n",
    "    policy_prior=None,\n",
    "    initial_policy=None,\n",
    "    check_convergence=True,\n",
    "    force_nonzero_probabilities=True,\n",
    ")\n",
    "\n",
    "# Max Entropy IRL expert policy\n",
    "expert_erpi2 = entropy_regularized_policy_iteration(\n",
    "    **expert_erpi_params2\n",
    ")\n",
    "\n",
    "expert_policy2 = TabularPolicy.from_matrix(\n",
    "    states=mdp2.state_list,\n",
    "    actions=mdp2.action_list,\n",
    "    policy_matrix=expert_erpi2.policy.detach().numpy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurizer2(state, action, next_state):\n",
    "  feature_name = mdp2.location_features.get(next_state, 's')\n",
    "  if feature_name in 's':\n",
    "    return {}\n",
    "  return {feature_name: 1}\n",
    "\n",
    "\n",
    "def fixed_reward2(state, action, next_state):\n",
    "  return mdp2.step_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm2 = MaxLikelihoodIRL(mdp2, featurizer2, fixed_reward2, batch_size=batch_size, epochs=epochs, lr=lr, weight_decay=weight_decay, momentum=momentum, entropy_weight=entropy_weight)\n",
    "\n",
    "# compute policy from learned weights\n",
    "feature_reward_matrix2 = torch.einsum(\n",
    "    \"sanf,f->san\",\n",
    "    algorithm2.get_feature_matrix(),\n",
    "    learned_weights\n",
    ")\n",
    "\n",
    "reward_matrix2 = feature_reward_matrix2 + algorithm2.get_fixed_reward_matrix()\n",
    "terminal_index = mdp2.state_index.get(\n",
    "    frozendict({'x': -1, 'y': -1}))\n",
    "reward_matrix2[:, :, terminal_index] = 0\n",
    "\n",
    "my_erpi_params2 = dict(\n",
    "    transition_matrix=torch.tensor(mdp2.transition_matrix),\n",
    "    reward_matrix=reward_matrix2,\n",
    "    discount_rate=torch.tensor(mdp2.discount_rate),\n",
    "    # the lower this is, the more optimal the policy\n",
    "    entropy_weight=torch.tensor([entropy_weight]),\n",
    "    n_planning_iters=10,\n",
    "    policy_prior=None,\n",
    "    initial_policy=None,\n",
    "    check_convergence=True,\n",
    "    force_nonzero_probabilities=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset2 = FeaturesDataset(mdp2, trajectories, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.eval()\n",
    "\n",
    "temp_imitpolicy2 = {}\n",
    "# so pred doesn't have a grad_fn attached to it\n",
    "with torch.no_grad():\n",
    "  for s in mdp2.state_list:\n",
    "    state_feature = torch.from_numpy(\n",
    "        np.array([features_dataset2.getStateFeature(s)], dtype='f')).to(device)\n",
    "    # pred = model(state_feature)[0]\n",
    "    pred = optimized_model(state_feature)\n",
    "    probs = softmax_fn(pred)[0]\n",
    "    temp_imitpolicy2[s] = {}\n",
    "    # for each action in each state, grab from the \"pred\" result the probability of taking this action in this state and add that to the dictionary policy which is a dictionary with states as keys and a dictionary of \"action: prob of taking this action in this state\" as values\n",
    "    for a_i, prob in enumerate(probs):\n",
    "      temp_imitpolicy2[s][int_to_action[a_i]] = prob.item()\n",
    "\n",
    "# to turn policy into an actual policy object that we can call the MSDM functions on\n",
    "imitation_policy2 = TabularPolicy({s: DictDistribution(ap)\n",
    "                       for s, ap in temp_imitpolicy2.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve with matrices returned by gridworld object\n",
    "my_erpi2 = entropy_regularized_policy_iteration(\n",
    "    **my_erpi_params2\n",
    ")\n",
    "\n",
    "max_policy2 = TabularPolicy.from_matrix(\n",
    "    states=mdp2.state_list,\n",
    "    actions=mdp2.action_list,\n",
    "    policy_matrix=my_erpi2.policy.detach().numpy()\n",
    ")\n",
    "\n",
    "mdp2.plot(featurecolors=featurecolors).plot_policy(max_policy2)\n",
    "mdp2.plot().plot_state_map(max_policy2.evaluate_on(mdp2).V)\n",
    "\n",
    "mdp2.plot(featurecolors=featurecolors).plot_policy(expert_policy2)\n",
    "mdp2.plot().plot_state_map(expert_policy2.evaluate_on(mdp2).V)\n",
    "\n",
    "mdp2.plot(featurecolors=featurecolors).plot_policy(imitation_policy2)\n",
    "mdp2.plot().plot_state_map(imitation_policy2.evaluate_on(mdp2).V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max policy initial value: {max_policy2.evaluate_on(mdp2).initial_value}\")\n",
    "print(f\"Expert policy initial value: {expert_policy2.evaluate_on(mdp2).initial_value}\")\n",
    "print(f\"Imitation policy initial Value: {imitation_policy2.evaluate_on(mdp2).initial_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a48d4cd50a2a48eca588c9887707ccbe5fc1fb45f33358dc08b670e55b95267a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('IRLIW': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
