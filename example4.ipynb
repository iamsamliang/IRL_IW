{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msdm.domains import GridWorld\n",
    "from msdm.algorithms.entregpolicyiteration import entropy_regularized_policy_iteration\n",
    "from msdm.core.problemclasses.mdp import TabularPolicy\n",
    "from msdm.core.distributions import DictDistribution\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from frozendict import frozendict\n",
    "from dataset import TrajectoryDataset, FeaturesDataset\n",
    "from algorithms import MaxLikelihoodIRL, ImitationLearning\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_trajs = 10000\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "lr = 1\n",
    "weight_decay = 0\n",
    "momentum = 0.9\n",
    "entropy_weight = 2\n",
    "discount_rate = 0.99\n",
    "step_cost = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<msdm.domains.gridworld.plotting.GridWorldPlotter at 0x7ff221785670>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2664x1512 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gridworld with 8 FEATURES not including goal and '.' states\n",
    "mdp = GridWorld(\n",
    "    tile_array=[\n",
    "        \"s.....r...r.uvvx...xx.ywzwxy.rrrrrrzg\",\n",
    "        \".utuxxrxyxrx.....x......z....xxvvxv..\",\n",
    "        \"....x.rrrxu...xx...xxx.......rr...rz.\",\n",
    "        \"x.x...txrxrrrr..........xxx...y.x....\",\n",
    "        \"..xx....uyrrrrx..wyyyxx..xxxx.....xzx\",\n",
    "        \"rrxx..x...rrrrxx...ww....z....tu..x..\",\n",
    "        \"rx...x..uuww..x.....v.xx...xxwxwv.rrr\",\n",
    "        \"tx.tvvx..x.xx....xx...xvvx..xwx.v.rrr\",\n",
    "        \"....vx..x.xt..x.....z...wyxyx.y.x....\",\n",
    "        \".x..v.x..t.yy.wx.y..y...xxtw......zz.\",\n",
    "        \"r..............x.v.tux..y.uv..x......\",\n",
    "        \"rrx..x.t.xwtz....x...x..v.y..vx...tvt\",\n",
    "        \".r..x....xrvx..x.vwxx...v.w..v.u..yvv\",\n",
    "        \"xrx...txwxrvy.............x....u..www\",\n",
    "        \".urx..rrrywxv.x..vvvvxx...rrr..t....x\",\n",
    "        \".xrr..x..y....xx.................z..w\",\n",
    "        \".x..v.x..x.....x..y...vx...x..x.....y\",\n",
    "        \"rx..v.rrrxrxu..t.txv..xv.x..uxxrrxrvz\",\n",
    "        \"z...x.v..xrx...u..u..x.w....tu.rwwuut\",\n",
    "        \".x..w.v..rrz..tx..wz..v...w.ttxrrrrrr\",\n",
    "        \"sx..t.v.x..xvuvy.......x....uttwyuwyz\",\n",
    "    ],\n",
    "    feature_rewards={\n",
    "        '.': 0,\n",
    "        'g': 0,\n",
    "        'r': 0.5,\n",
    "        't': -1,\n",
    "        'u': -2,\n",
    "        'v': -5,\n",
    "        'w': -7,\n",
    "        'x': -10,\n",
    "        'y': -13,\n",
    "        'z': -20,\n",
    "    },\n",
    "    absorbing_features=('g'),\n",
    "    initial_features=('s'),\n",
    "    discount_rate=discount_rate,\n",
    "    step_cost=step_cost,\n",
    ")\n",
    "featurecolors = {\n",
    "    '.': 'white',\n",
    "    'g': 'green',\n",
    "    'r': 'lightgreen',\n",
    "    't': 'grey',\n",
    "    'u': 'purple',\n",
    "    'v': 'magenta',\n",
    "    'w': 'yellow',\n",
    "    'x': 'black',\n",
    "    'y': 'orange',\n",
    "    'z': 'red',\n",
    "}\n",
    "mdp.plot(featurecolors=featurecolors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Maximum Entropy IRL policy to generate trajectories\n",
    "expert_erpi_params = dict(\n",
    "    transition_matrix=torch.tensor(mdp.transition_matrix),\n",
    "    reward_matrix=torch.tensor(mdp.reward_matrix),\n",
    "    discount_rate=torch.tensor(mdp.discount_rate),\n",
    "    # the lower this is, the more optimal the policy\n",
    "    entropy_weight=torch.tensor([entropy_weight]),\n",
    "    n_planning_iters=10,\n",
    "    policy_prior=None,\n",
    "    initial_policy=None,\n",
    "    check_convergence=True,\n",
    "    force_nonzero_probabilities=True,\n",
    ")\n",
    "\n",
    "# Max Entropy IRL expert policy\n",
    "expert_erpi = entropy_regularized_policy_iteration(\n",
    "    **expert_erpi_params\n",
    ")\n",
    "\n",
    "expert_policy = TabularPolicy.from_matrix(\n",
    "    states=mdp.state_list,\n",
    "    actions=mdp.action_list,\n",
    "    policy_matrix=expert_erpi.policy.detach().numpy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our numpy 2d array to a tensor for training\n",
    "# do I need to convert the labels to a tensor? yes, does it for us in Lambda (target_transform)\n",
    "class ToTensor(object):\n",
    "  \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "  def __call__(self, sample):\n",
    "    return torch.from_numpy(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "ActionClassifier(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "class ActionClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ActionClassifier, self).__init__()\n",
    "    self.conv1 = nn.Sequential(\n",
    "        nn.Conv1d(1, 16, 3, padding=1),\n",
    "        nn.BatchNorm1d(16),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.conv2 = nn.Sequential(\n",
    "        nn.Conv1d(16, 32, 3, padding=1),\n",
    "        nn.BatchNorm1d(32),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.conv3 = nn.Sequential(\n",
    "        nn.Conv1d(32, 64, 3, stride=2, padding=1),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 5),\n",
    "    )\n",
    "    # don't use softmax bc nn.CrossEntropyLoss takes unnormalized outputs\n",
    "    # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_conv1 = self.conv1(x)\n",
    "    x_conv2 = self.conv2(x_conv1)\n",
    "    x_conv3 = self.conv3(x_conv2)\n",
    "    x_flat = self.flatten(x_conv3)\n",
    "    logits = self.linear_relu_stack(x_flat)\n",
    "    # probabilities = self.softmax(logits)\n",
    "    return logits\n",
    "\n",
    "\n",
    "model = ActionClassifier().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate gen_iters of trajectories\n",
    "gen_iters = num_trajs\n",
    "trajectories = []\n",
    "\n",
    "for _ in range(gen_iters):\n",
    "  # Starts at one of the labelled starting points\n",
    "  trajectory = expert_policy.run_on(mdp)\n",
    "  formatted_traj = {}\n",
    "  formatted_traj['state_traj'] = trajectory.state_traj\n",
    "  formatted_traj['action_traj'] = trajectory.action_traj\n",
    "  trajectories.append(formatted_traj)\n",
    "\n",
    "trajs_dataset = TrajectoryDataset(trajectories)\n",
    "features_dataset = FeaturesDataset(mdp, trajectories, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cross Entropy Loss for classification\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# better optimizer with a scheduler to decrease learning rate by 0.1 at indicated steps\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.1, weight_decay=0.0001, momentum=momentum)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, [10, 20, 30, 40], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurizer(state, action, next_state):\n",
    "  feature_name = mdp.location_features.get(next_state, 's')\n",
    "  if feature_name in 's':\n",
    "    return {}\n",
    "  return {feature_name: 1}\n",
    "\n",
    "def fixed_reward(state, action, next_state):\n",
    "  return mdp.step_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = MaxLikelihoodIRL(mdp, featurizer, fixed_reward, batch_size=batch_size, epochs=epochs,\n",
    "                             lr=lr, momentum=momentum, entropy_weight=entropy_weight, weight_decay=weight_decay)\n",
    "\n",
    "supervised_learning = ImitationLearning(\n",
    "    mdp, _, _, model, loss_fn, optimizer, scheduler=scheduler, batch_size=batch_size, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n",
      "Inital reward weights: tensor([-1.4185,  1.2404,  0.2024, -0.2852, -0.7467], dtype=torch.float64)\n",
      "\n",
      "loss: 2.322097  [    0/10000]\n",
      "loss: 1.724695  [    0/10000]\n",
      "loss: 1.632993  [    0/10000]\n",
      "loss: 1.740636  [    0/10000]\n",
      "loss: 1.749771  [    0/10000]\n",
      "loss: 1.866481  [    0/10000]\n",
      "loss: 1.830929  [    0/10000]\n",
      "loss: 1.615136  [    0/10000]\n",
      "loss: 1.879451  [    0/10000]\n",
      "loss: 1.694390  [    0/10000]\n",
      "loss: 1.758329  [    0/10000]\n",
      "loss: 1.750879  [    0/10000]\n",
      "loss: 1.780777  [    0/10000]\n",
      "loss: 1.685266  [    0/10000]\n",
      "loss: 1.668990  [    0/10000]\n",
      "loss: 1.897092  [    0/10000]\n",
      "loss: 1.694206  [    0/10000]\n",
      "loss: 1.749220  [    0/10000]\n",
      "loss: 1.786807  [    0/10000]\n",
      "loss: 1.838805  [    0/10000]\n",
      "loss: 1.663982  [    0/10000]\n",
      "loss: 1.822361  [    0/10000]\n",
      "loss: 1.725878  [    0/10000]\n",
      "loss: 1.609854  [    0/10000]\n",
      "loss: 1.640273  [    0/10000]\n",
      "loss: 1.738591  [    0/10000]\n",
      "loss: 1.726312  [    0/10000]\n",
      "loss: 1.771751  [    0/10000]\n",
      "loss: 1.795881  [    0/10000]\n"
     ]
    }
   ],
   "source": [
    "learned_weights, the_policy, max_losses = algorithm.learn(trajs_dataset)\n",
    "optimized_model, imitation_losses = supervised_learning.learn(features_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve with matrices returned by gridworld object\n",
    "max_policy = TabularPolicy.from_matrix(\n",
    "    states=mdp.state_list,\n",
    "    actions=mdp.action_list,\n",
    "    policy_matrix=the_policy.policy.detach().numpy()\n",
    ")\n",
    "\n",
    "mdp.plot(featurecolors=featurecolors).plot_policy(max_policy)\n",
    "mdp.plot().plot_state_map(max_policy.evaluate_on(mdp).V)\n",
    "\n",
    "mdp.plot(featurecolors=featurecolors).plot_policy(expert_policy)\n",
    "mdp.plot().plot_state_map(expert_policy.evaluate_on(mdp).V)\n",
    "\n",
    "optimized_model.eval()\n",
    "\n",
    "temp_imitpolicy = {}\n",
    "int_to_action = {0: frozendict(dx=0, dy=1), 1: frozendict(\n",
    "    dx=0, dy=-1), 2: frozendict(dx=1, dy=0), 3: frozendict(dx=-1, dy=0), 4: frozendict(dx=0, dy=0)}\n",
    "\n",
    "softmax_fn = nn.Softmax(dim=1)\n",
    "\n",
    "# so pred doesn't have a grad_fn attached to it\n",
    "with torch.no_grad():\n",
    "  for s in mdp.state_list:\n",
    "    state_feature = torch.from_numpy(\n",
    "        np.array([features_dataset.getStateFeature(s)], dtype='f')).to(device)\n",
    "    # pred = model(state_feature)[0]\n",
    "    # pred = supervised_learning.model(state_feature)\n",
    "    pred = optimized_model(state_feature)\n",
    "    probs = softmax_fn(pred)[0]\n",
    "    temp_imitpolicy[s] = {}\n",
    "    # for each action in each state, grab from the \"pred\" result the probability of taking this action in this state and add that to the dictionary policy which is a dictionary with states as keys and a dictionary of \"action: prob of taking this action in this state\" as values\n",
    "    for a_i, prob in enumerate(probs):\n",
    "      temp_imitpolicy[s][int_to_action[a_i]] = prob.item()\n",
    "\n",
    "# to turn policy into an actual policy object that we can call the MSDM functions on\n",
    "imitation_policy = TabularPolicy({s: DictDistribution(ap) for s, ap in temp_imitpolicy.items()})\n",
    "\n",
    "mdp.plot(featurecolors=featurecolors).plot_policy(imitation_policy)\n",
    "mdp.plot().plot_state_map(imitation_policy.evaluate_on(mdp).V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max policy initial value: {max_policy.evaluate_on(mdp).initial_value}\")\n",
    "print(f\"Expert policy initial value: {expert_policy.evaluate_on(mdp).initial_value}\")\n",
    "print(f\"Imitation policy initial value: {imitation_policy.evaluate_on(mdp).initial_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gridworld\n",
    "mdp2 = GridWorld(\n",
    "    tile_array=[\n",
    "        \"tvvtvyyr....z.yruwx.x.uxxxwvy..............rrrx\",\n",
    "        \"tzwyu.x..yv.yvw...wrtwxu...wz.yyyw.wwvy..wx..ru\",\n",
    "        \"wyrrrrrr..x.ytt.v..wr....x....y.yzztzzygyx..xy.\",\n",
    "        \"y.rxwrz..yz..rrrzx.rw.yzvuuwzuwwuv.yxzuzz..zuvr\",\n",
    "        \"wxrzxu..tvzwyurtww....yuwyrr.wvx.rtuz.......wvt\",\n",
    "        \"..rzrzz....rzzyyxwtxruwxwwzuutzr.zwuruwvzrrytyr\",\n",
    "        \"wyrtt.twuz.vrzwvyzyzwwzxzuzxtww...uurxtxyrwrrr.\",\n",
    "        \"rtrwzzzz...uuww.zywzrwvzwx.yvvy.w...yzzw..rwtrv\",\n",
    "        \"uyrzuzzy.wwvztwrvy.wzyvwwzw..zz..zy.ux...zrry.t\",\n",
    "        \"rwrrrrrrrryryyyv.rwu.zuyyuzwwxt.xzt....vyzrrrr.\",\n",
    "        \"tw.uryrrsrr.tuxrtzwz........trt.twrvu...tu.xxyv\",\n",
    "        \"zy.wrtrrrrr.xxtx.uzt.vxvru.txwx.v.ry..wt.xuvxvr\",\n",
    "        \"rt.wr..wury.....yvv...v.zu......vu.xyrrrr.uuwxt\",\n",
    "        \"xr.ywzvwruuutzy.x.y..r.zrvztxz.ryvvtyuz.x.xxvtz\",\n",
    "        \"rwrrtttywrvrvzy.w.ty.tyyyx.r.r..uw...xxvu.ytxy.\",\n",
    "        \"yztrtvz.t.uzyr..wwyr.t.yvz.uyur.zy.z......vvvtu\",\n",
    "        \"wuz.uwzvv.vyu.z............rzyx....wx.yyvrzr.x.\",\n",
    "        \"utzrrr.utr.yu.wvtvuz.v..vz.rrrrwywvu.rzwtu.z.yv\",\n",
    "        \"yvzw...yt.tzur.uuz.yty.rt.vxwtrz.rtw.ryxxtvxrvy\",\n",
    "        \"ywy..vrwy.vwxw.w..uru..zrvwutr..zxxz.yxtuvzvyvz\",\n",
    "        \"ytzw.y.........xrxuytv.zztuvzvrwzytw.rr..ztzwvt\",\n",
    "        \"z..r.wr.trytrrrvrzywuvrr.trtzy..tvvv.tvx.wwrt.y\",\n",
    "        \"uwu..yuzuzrwrrr.r.rr.rr...r.rrrrrr...wyt.wvrzuv\",\n",
    "        \"yu.......r...vvr.wy..urz.w...v.y.w.zw.ru.vuxtt.\",\n",
    "        \".w..s..x.z.tw.v..zw.y.rt..u.tu.x.t....zr.wx.vu.\",\n",
    "        \"vyrrrrv.wy..v.r..v.z.zr.vyz..rrr...s.......xvvx\",\n",
    "        \"r...rrr.....rrrt..r..rr...w..w.vr...ry...rr.rwy\",\n",
    "    ],\n",
    "    feature_rewards={\n",
    "        '.': 0,\n",
    "        'g': 0,\n",
    "        'r': 0.5,\n",
    "        't': -1,\n",
    "        'u': -2,\n",
    "        'v': -5,\n",
    "        'w': -7,\n",
    "        'x': -10,\n",
    "        'y': -13,\n",
    "        'z': -20,\n",
    "    },\n",
    "    absorbing_features=('g'),\n",
    "    initial_features=('s'),\n",
    "    discount_rate=discount_rate,\n",
    "    step_cost=step_cost,\n",
    ")\n",
    "\n",
    "mdp2.plot(featurecolors=featurecolors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_erpi_params2 = dict(\n",
    "    transition_matrix=torch.tensor(mdp2.transition_matrix),\n",
    "    reward_matrix=torch.tensor(mdp2.reward_matrix),\n",
    "    discount_rate=torch.tensor(mdp2.discount_rate),\n",
    "    # the lower this is, the more optimal the policy\n",
    "    entropy_weight=torch.tensor([entropy_weight]),\n",
    "    n_planning_iters=10,\n",
    "    policy_prior=None,\n",
    "    initial_policy=None,\n",
    "    check_convergence=True,\n",
    "    force_nonzero_probabilities=True,\n",
    ")\n",
    "\n",
    "# Max Entropy IRL expert policy\n",
    "expert_erpi2 = entropy_regularized_policy_iteration(\n",
    "    **expert_erpi_params2\n",
    ")\n",
    "\n",
    "expert_policy2 = TabularPolicy.from_matrix(\n",
    "    states=mdp2.state_list,\n",
    "    actions=mdp2.action_list,\n",
    "    policy_matrix=expert_erpi2.policy.detach().numpy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurizer2(state, action, next_state):\n",
    "  feature_name = mdp2.location_features.get(next_state, 's')\n",
    "  if feature_name in 's':\n",
    "    return {}\n",
    "  return {feature_name: 1}\n",
    "\n",
    "\n",
    "def fixed_reward2(state, action, next_state):\n",
    "  return mdp2.step_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm2 = MaxLikelihoodIRL(mdp2, featurizer2, fixed_reward2, batch_size=batch_size, epochs=epochs, lr=lr, weight_decay=weight_decay, momentum=momentum, entropy_weight=entropy_weight)\n",
    "\n",
    "# compute policy from learned weights\n",
    "feature_reward_matrix2 = torch.einsum(\n",
    "    \"sanf,f->san\",\n",
    "    algorithm2.get_feature_matrix(),\n",
    "    learned_weights\n",
    ")\n",
    "\n",
    "reward_matrix2 = feature_reward_matrix2 + algorithm2.get_fixed_reward_matrix()\n",
    "terminal_index = mdp2.state_index.get(\n",
    "    frozendict({'x': -1, 'y': -1}))\n",
    "reward_matrix2[:, :, terminal_index] = 0\n",
    "\n",
    "my_erpi_params2 = dict(\n",
    "    transition_matrix=torch.tensor(mdp2.transition_matrix),\n",
    "    reward_matrix=reward_matrix2,\n",
    "    discount_rate=torch.tensor(mdp2.discount_rate),\n",
    "    # the lower this is, the more optimal the policy\n",
    "    entropy_weight=torch.tensor([entropy_weight]),\n",
    "    n_planning_iters=10,\n",
    "    policy_prior=None,\n",
    "    initial_policy=None,\n",
    "    check_convergence=True,\n",
    "    force_nonzero_probabilities=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset2 = FeaturesDataset(mdp2, trajectories, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.eval()\n",
    "\n",
    "temp_imitpolicy2 = {}\n",
    "# so pred doesn't have a grad_fn attached to it\n",
    "with torch.no_grad():\n",
    "  for s in mdp2.state_list:\n",
    "    state_feature = torch.from_numpy(\n",
    "        np.array([features_dataset2.getStateFeature(s)], dtype='f')).to(device)\n",
    "    # pred = model(state_feature)[0]\n",
    "    pred = optimized_model(state_feature)\n",
    "    probs = softmax_fn(pred)[0]\n",
    "    temp_imitpolicy2[s] = {}\n",
    "    # for each action in each state, grab from the \"pred\" result the probability of taking this action in this state and add that to the dictionary policy which is a dictionary with states as keys and a dictionary of \"action: prob of taking this action in this state\" as values\n",
    "    for a_i, prob in enumerate(probs):\n",
    "      temp_imitpolicy2[s][int_to_action[a_i]] = prob.item()\n",
    "\n",
    "# to turn policy into an actual policy object that we can call the MSDM functions on\n",
    "imitation_policy2 = TabularPolicy({s: DictDistribution(ap)\n",
    "                       for s, ap in temp_imitpolicy2.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve with matrices returned by gridworld object\n",
    "my_erpi2 = entropy_regularized_policy_iteration(\n",
    "    **my_erpi_params2\n",
    ")\n",
    "\n",
    "max_policy2 = TabularPolicy.from_matrix(\n",
    "    states=mdp2.state_list,\n",
    "    actions=mdp2.action_list,\n",
    "    policy_matrix=my_erpi2.policy.detach().numpy()\n",
    ")\n",
    "\n",
    "mdp2.plot(featurecolors=featurecolors).plot_policy(max_policy2)\n",
    "mdp2.plot().plot_state_map(max_policy2.evaluate_on(mdp2).V)\n",
    "\n",
    "mdp2.plot(featurecolors=featurecolors).plot_policy(expert_policy2)\n",
    "mdp2.plot().plot_state_map(expert_policy2.evaluate_on(mdp2).V)\n",
    "\n",
    "mdp2.plot(featurecolors=featurecolors).plot_policy(imitation_policy2)\n",
    "mdp2.plot().plot_state_map(imitation_policy2.evaluate_on(mdp2).V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max policy initial value: {max_policy2.evaluate_on(mdp2).initial_value}\")\n",
    "print(f\"Expert policy initial value: {expert_policy2.evaluate_on(mdp2).initial_value}\")\n",
    "print(f\"Imitation policy initial Value: {imitation_policy2.evaluate_on(mdp2).initial_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a48d4cd50a2a48eca588c9887707ccbe5fc1fb45f33358dc08b670e55b95267a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('IRLIW': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
