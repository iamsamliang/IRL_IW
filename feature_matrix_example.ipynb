{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from msdm.domains import GridWorld\n",
    "from msdm.core.problemclasses.mdp import TabularPolicy\n",
    "from msdm.algorithms import PolicyIteration\n",
    "from msdm.algorithms.entregpolicyiteration import entropy_regularized_policy_iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_params = dict(\n",
    "    tile_array=[\n",
    "        \"..b.g\",\n",
    "        \"..b..\",\n",
    "        \"aa.aa\",\n",
    "        \"..b..\",\n",
    "        \"s.b..\",\n",
    "    ],\n",
    "    feature_rewards={\n",
    "        \"a\": -100,\n",
    "        \"b\": 0,\n",
    "    },\n",
    "    discount_rate=1-1e-5,\n",
    "    step_cost=-1\n",
    "\n",
    ")\n",
    "gw = GridWorld(**gw_params)\n",
    "\n",
    "erpi_params = dict(\n",
    "    transition_matrix=torch.tensor(gw.transition_matrix),\n",
    "    reward_matrix=torch.tensor(gw.reward_matrix),\n",
    "    discount_rate=torch.tensor(gw.discount_rate),\n",
    "    # the lower this is, the more optimal the policy\n",
    "    entropy_weight=torch.tensor([5]),\n",
    "    n_planning_iters=10,\n",
    "    policy_prior=None,\n",
    "    initial_policy=None,\n",
    "    check_convergence=True,\n",
    "    force_nonzero_probabilities=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve with matrices returned by gridworld object\n",
    "original_erpi = entropy_regularized_policy_iteration(\n",
    "    **erpi_params\n",
    ")\n",
    "policy = TabularPolicy.from_matrix(\n",
    "    states=gw.state_list,\n",
    "    actions=gw.action_list,\n",
    "    policy_matrix=original_erpi.policy.detach().numpy()\n",
    ")\n",
    "state_values = dict(zip(gw.state_list, original_erpi.state_values))\n",
    "gw.plot(featurecolors={'a': 'r', 'b': 'g', 'g': 'yellow'}).plot_policy(policy)\n",
    "gw.plot().plot_state_map(state_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an equivalent reward function with feature matrices\n",
    "def create_feature_matrix(gw, features):\n",
    "    state_feature_matrix = torch.zeros(len(gw.state_list), len(features))\n",
    "    for si, s in enumerate(gw.state_list):\n",
    "        f = gw.location_features.get(s, '.')\n",
    "        if f in features:\n",
    "            fi = features.index(f)\n",
    "        else:\n",
    "            continue\n",
    "        state_feature_matrix[si, fi] = 1\n",
    "    state_action_nextstate_feature_matrix = state_feature_matrix[None, None, :]\n",
    "    return state_action_nextstate_feature_matrix\n",
    "\n",
    "\n",
    "def create_step_cost_matrix(gw):\n",
    "    # action at every state has a step cost, except the terminal state\n",
    "    state_step_cost_matrix = torch.tensor(\n",
    "        gw.step_cost)*gw.nonterminal_state_vec\n",
    "    step_cost_matrix = state_step_cost_matrix[:, None, None]\n",
    "    return step_cost_matrix\n",
    "\n",
    "\n",
    "# get features and feature reward_weights\n",
    "features = sorted(gw_params['feature_rewards'].keys())\n",
    "features = [f for f in features if f not in 'gs']\n",
    "feature_reward_weights = torch.tensor(\n",
    "    [float(gw_params['feature_rewards'][f]) for f in features])\n",
    "\n",
    "# create reward_matrix from features\n",
    "feature_matrix = create_feature_matrix(gw, features)\n",
    "feature_reward_matrix = torch.einsum(\n",
    "    \"sanf,f->san\",\n",
    "    feature_matrix,\n",
    "    feature_reward_weights\n",
    ").double()\n",
    "step_cost_matrix = create_step_cost_matrix(gw)\n",
    "reward_matrix = feature_reward_matrix + step_cost_matrix\n",
    "\n",
    "# this should be |S| x 1 x |S| since its the same for all actions\n",
    "assert tuple(reward_matrix.shape) == (\n",
    "    len(gw.state_list), 1, len(gw.state_list))\n",
    "# anything leading to the terminal state ({-1, -1}) has zero reward\n",
    "reward_matrix[:, :, 0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_erpi = entropy_regularized_policy_iteration(**{\n",
    "    **erpi_params,\n",
    "    \"reward_matrix\": reward_matrix  # this overrides the entry in erpi_params\n",
    "})\n",
    "policy = TabularPolicy.from_matrix(\n",
    "    states=gw.state_list,\n",
    "    actions=gw.action_list,\n",
    "    policy_matrix=new_erpi.policy.detach().numpy()\n",
    ")\n",
    "state_values = dict(zip(gw.state_list, new_erpi.state_values))\n",
    "gw.plot(featurecolors={'a': 'r', 'b': 'g', 'g': 'yellow'}).plot_policy(policy)\n",
    "gw.plot().plot_state_map(state_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the reward function is exactly the same, so the resulting state values\n",
    "# should be exactly the same\n",
    "assert (original_erpi.state_values - new_erpi.state_values).sum() == 0\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
