{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msdm.domains import GridWorld\n",
    "from msdm.algorithms.entregpolicyiteration import entropy_regularized_policy_iteration\n",
    "from msdm.core.problemclasses.mdp import TabularPolicy\n",
    "from msdm.core.distributions import DictDistribution\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from frozendict import frozendict\n",
    "from dataset import TrajectoryDataset, FeaturesDataset\n",
    "from algorithms import MaxLikelihoodIRL, ImitationLearning\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_trajs = 100000\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "lr = 1\n",
    "weight_decay = 0\n",
    "momentum = 0.9\n",
    "entropy_weight = 2\n",
    "discount_rate = 0.99\n",
    "step_cost = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<msdm.domains.gridworld.plotting.GridWorldPlotter at 0x28c9f076100>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2664x1584 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a gridworld\n",
    "mdp = GridWorld(\n",
    "    tile_array=[\n",
    "        \"sx....c.c..c.bbx...xx.xx.xxx.bx..bc.g\",\n",
    "        \".axcxc.xxx.x.bbc.x..aaaa....xxx...x..\",\n",
    "        \"c...x....xbac.xx...xxx.....bbb....c.b\",\n",
    "        \"x.x.a.xx.x....c..ab.....xxx.......c.a\",\n",
    "        \"..xxaa....c...x..ba..xx....ac.....x.x\",\n",
    "        \"..xx..xaaa..cabx........abbc......x..\",\n",
    "        \".x.bb.x..xx.xccb..ba..cx..axbbx.ab..c\",\n",
    "        \".c.b.ax..x.a..cx.xx...xcxx..xbxc.....\",\n",
    "        \"...bbax..x.cxbcx......xbxab.b...bbb..\",\n",
    "        \".x.b.ax..a..aa....xx..xab.x..xx..aaa.\",\n",
    "        \".x..bax..x.x.xcx...ab...xxcx.a...cbcc\",\n",
    "        \".a....c.c..b.bbx.....xabccxx.bx..bc.a\",\n",
    "        \".axbcxcx.x.c..bc.x..axaa....ccx...b..\",\n",
    "        \"c...x....xbac.cx...xxabc...bbb....a.b\",\n",
    "        \"x.x...xb.x....c..ab.ccab..x....c..c.a\",\n",
    "        \"..axca....c...x..ba..xx....ac..c..b.x\",\n",
    "        \"..cc..xcca..caxx....c...abbc......a..\",\n",
    "        \".c.ab.x..x.cc.ax..ba..xx..axbcx.ab..c\",\n",
    "        \".c...aa..x.xxba..xx...xxcx..xxxc.....\",\n",
    "        \"....xac..x.xabaa......xcxabbb...ccc..\",\n",
    "        \".c.a.ab..a..aaxx..xx..xab.x..xx..aab.\",\n",
    "        \"sx......x.axaabx...b...xccb.ab..cccx.\",\n",
    "    ],\n",
    "    feature_rewards={\n",
    "        '.': 0,\n",
    "        'a': 0,\n",
    "        'g': 10,\n",
    "        'x': -12,\n",
    "        'c': -5,\n",
    "        'b': -1,\n",
    "    },\n",
    "    absorbing_features=('g'),\n",
    "    initial_features=('s'),\n",
    "    discount_rate=discount_rate,\n",
    "    step_cost=step_cost,\n",
    ")\n",
    "featurecolors = {\n",
    "    '.': 'white',\n",
    "    'a': 'lightgreen',\n",
    "    'g': 'green',\n",
    "    'x': 'red',\n",
    "    'c': 'black',\n",
    "    'b': 'blue'\n",
    "}\n",
    "mdp.plot(featurecolors=featurecolors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Maximum Entropy IRL policy to generate trajectories\n",
    "expert_erpi_params = dict(\n",
    "    transition_matrix=torch.tensor(mdp.transition_matrix),\n",
    "    reward_matrix=torch.tensor(mdp.reward_matrix),\n",
    "    discount_rate=torch.tensor(mdp.discount_rate),\n",
    "    # the lower this is, the more optimal the policy\n",
    "    entropy_weight=torch.tensor([entropy_weight]),\n",
    "    n_planning_iters=10,\n",
    "    policy_prior=None,\n",
    "    initial_policy=None,\n",
    "    check_convergence=True,\n",
    "    force_nonzero_probabilities=True,\n",
    ")\n",
    "\n",
    "# Max Entropy IRL expert policy\n",
    "expert_erpi = entropy_regularized_policy_iteration(\n",
    "    **expert_erpi_params\n",
    ")\n",
    "\n",
    "expert_policy = TabularPolicy.from_matrix(\n",
    "    states=mdp.state_list,\n",
    "    actions=mdp.action_list,\n",
    "    policy_matrix=expert_erpi.policy.detach().numpy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our numpy 2d array to a tensor for training\n",
    "# do I need to convert the labels to a tensor? yes, does it for us in Lambda (target_transform)\n",
    "class ToTensor(object):\n",
    "  \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "  def __call__(self, sample):\n",
    "    return torch.from_numpy(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "ActionClassifier(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "class ActionClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ActionClassifier, self).__init__()\n",
    "    self.conv1 = nn.Sequential(\n",
    "        nn.Conv1d(1, 16, 3, padding=1),\n",
    "        nn.BatchNorm1d(16),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.conv2 = nn.Sequential(\n",
    "        nn.Conv1d(16, 32, 3, padding=1),\n",
    "        nn.BatchNorm1d(32),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.conv3 = nn.Sequential(\n",
    "        nn.Conv1d(32, 64, 3, stride=2, padding=1),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 5),\n",
    "    )\n",
    "    # don't use softmax bc nn.CrossEntropyLoss takes unnormalized outputs\n",
    "    # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_conv1 = self.conv1(x)\n",
    "    x_conv2 = self.conv2(x_conv1)\n",
    "    x_conv3 = self.conv3(x_conv2)\n",
    "    x_flat = self.flatten(x_conv3)\n",
    "    logits = self.linear_relu_stack(x_flat)\n",
    "    # probabilities = self.softmax(logits)\n",
    "    return logits\n",
    "\n",
    "\n",
    "model = ActionClassifier().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate gen_iters of trajectories\n",
    "gen_iters = num_trajs\n",
    "trajectories = []\n",
    "\n",
    "for _ in range(gen_iters):\n",
    "  # Starts at one of the labelled starting points\n",
    "  trajectory = expert_policy.run_on(mdp)\n",
    "  formatted_traj = {}\n",
    "  formatted_traj['state_traj'] = trajectory.state_traj\n",
    "  formatted_traj['action_traj'] = trajectory.action_traj\n",
    "  trajectories.append(formatted_traj)\n",
    "\n",
    "trajs_dataset = TrajectoryDataset(trajectories)\n",
    "features_dataset = FeaturesDataset(mdp, trajectories, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cross Entropy Loss for classification\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# better optimizer with a scheduler to decrease learning rate by 0.1 at indicated steps\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.1, weight_decay=0.0001, momentum=momentum)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, [10, 20, 30, 40], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurizer(state, action, next_state):\n",
    "  feature_name = mdp.location_features.get(next_state, 's')\n",
    "  if feature_name in 's':\n",
    "    return {}\n",
    "  return {feature_name: 1}\n",
    "\n",
    "def fixed_reward(state, action, next_state):\n",
    "  return mdp.step_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = MaxLikelihoodIRL(mdp, featurizer, fixed_reward, batch_size=batch_size, epochs=epochs,\n",
    "                             lr=lr, momentum=momentum, entropy_weight=entropy_weight, weight_decay=weight_decay)\n",
    "\n",
    "supervised_learning = ImitationLearning(\n",
    "    mdp, _, _, model, loss_fn, optimizer, scheduler=scheduler, batch_size=batch_size, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Inital reward weights: tensor([ 2.2772, -0.6890, -0.2900, -1.0179, -0.0981], dtype=torch.float64)\n",
      "\n",
      "loss: 0.315839  [    0/100000]\n"
     ]
    }
   ],
   "source": [
    "learned_weights, the_policy, max_losses = algorithm.learn(trajs_dataset)\n",
    "optimized_model, imitation_losses = supervised_learning.learn(features_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve with matrices returned by gridworld object\n",
    "max_policy = TabularPolicy.from_matrix(\n",
    "    states=mdp.state_list,\n",
    "    actions=mdp.action_list,\n",
    "    policy_matrix=the_policy.policy.detach().numpy()\n",
    ")\n",
    "\n",
    "mdp.plot(featurecolors=featurecolors).plot_policy(max_policy)\n",
    "mdp.plot().plot_state_map(max_policy.evaluate_on(mdp).V)\n",
    "\n",
    "mdp.plot(featurecolors=featurecolors).plot_policy(expert_policy)\n",
    "mdp.plot().plot_state_map(expert_policy.evaluate_on(mdp).V)\n",
    "\n",
    "optimized_model.eval()\n",
    "\n",
    "temp_imitpolicy = {}\n",
    "int_to_action = {0: frozendict(dx=0, dy=1), 1: frozendict(\n",
    "    dx=0, dy=-1), 2: frozendict(dx=1, dy=0), 3: frozendict(dx=-1, dy=0), 4: frozendict(dx=0, dy=0)}\n",
    "\n",
    "softmax_fn = nn.Softmax(dim=1)\n",
    "\n",
    "# so pred doesn't have a grad_fn attached to it\n",
    "with torch.no_grad():\n",
    "  for s in mdp.state_list:\n",
    "    state_feature = torch.from_numpy(\n",
    "        np.array([features_dataset.getStateFeature(s)], dtype='f')).to(device)\n",
    "    # pred = model(state_feature)[0]\n",
    "    # pred = supervised_learning.model(state_feature)\n",
    "    pred = optimized_model(state_feature)\n",
    "    probs = softmax_fn(pred)[0]\n",
    "    temp_imitpolicy[s] = {}\n",
    "    # for each action in each state, grab from the \"pred\" result the probability of taking this action in this state and add that to the dictionary policy which is a dictionary with states as keys and a dictionary of \"action: prob of taking this action in this state\" as values\n",
    "    for a_i, prob in enumerate(probs):\n",
    "      temp_imitpolicy[s][int_to_action[a_i]] = prob.item()\n",
    "\n",
    "# to turn policy into an actual policy object that we can call the MSDM functions on\n",
    "imitation_policy = TabularPolicy({s: DictDistribution(ap) for s, ap in temp_imitpolicy.items()})\n",
    "\n",
    "mdp.plot(featurecolors=featurecolors).plot_policy(imitation_policy)\n",
    "mdp.plot().plot_state_map(imitation_policy.evaluate_on(mdp).V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max policy initial value: {max_policy.evaluate_on(mdp).initial_value}\")\n",
    "print(f\"Expert policy initial value: {expert_policy.evaluate_on(mdp).initial_value}\")\n",
    "print(f\"Imitation policy initial value: {imitation_policy.evaluate_on(mdp).initial_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gridworld\n",
    "mdp2 = GridWorld(\n",
    "    tile_array=[\n",
    "        \".x....c.c..c.x....xx....ca...x.x.a........g\",\n",
    "        \".x..xx....x....a......x....x.a...xx..abc.a.\",\n",
    "        \"...cxx.ccc.x.b.x.x....bx.abc.axa.a..caa..cx\",\n",
    "        \"..axx.ax.xax.xaxba.x.acc..a.abax.b...bac..x\",\n",
    "        \"a.ba.xx.ca.abb..xxxx.....xab.xaa.c.aaa.a.bc\",\n",
    "        \"cc.xaacc..xaccb.baaa.c.c.aaa.a.a.cccxba.cba\",\n",
    "        \"xabbbc.abcx.aaa..bb....ccxx..aa..x.cccbca.a\",\n",
    "        \"abaaa...cxxxx.aaab.abc.xxx......xa.x.bbba.a\",\n",
    "        \".abc.xaaaab....cx..abca.abbc...a.aca.ab.a.b\",\n",
    "        \"x.cabax.a...c..a..cc..abcxxaa...xxc..cbaccc\",\n",
    "        \".xacca...a..xxx.......xcb.a..x...xxx.xxcxxx\",\n",
    "        \".x..bax..x.xabcxxx.ab...xabx........caaaabb\",\n",
    "        \".a..xxabc...abbb.ac..xx..bx..bcxbbxxa...b.x\",\n",
    "        \".xaxcaxa.ax..xca.a.xaaac....abx...c.xaa.a.x\",\n",
    "        \"a.x.b....bxca..x...xb..a...ccx....a.babb.cb\",\n",
    "        \"x.x..abcc.....c..ab...ab.......cccc.a.b.b.b\",\n",
    "        \"..xaac.x..a......xxx.......ca..x....xaaa...\",\n",
    "        \"..cc..xcca..xcab....c...abbc......a..bc.a.b\",\n",
    "        \"x..xx.a..x....xb..xx...x..xacab.xx..cbababb\",\n",
    "        \".....xx..x.xxbx..xb...abcx..abcx....x...ccc\",\n",
    "        \"....a......ccba.c.ccc.acaxxx..a..cc...csc..\",\n",
    "        \".c.a...c.a........aa..xab.x..xx..aab....x..\",\n",
    "        \".x.abc..x.xcabcx.x.b...xccb.ab..cccx...cc..\",\n",
    "        \"sx..bax..xb.xx.ab....cc...a...abcbbb.......\",\n",
    "    ],\n",
    "    feature_rewards={\n",
    "        '.': 0,\n",
    "        'a': 0,\n",
    "        'g': 10,\n",
    "        'x': -12,\n",
    "        'c': -5,\n",
    "        'b': -1,\n",
    "    }\n",
    "    absorbing_features=('g'),\n",
    "    initial_features=('s'),\n",
    "    discount_rate=discount_rate,\n",
    "    step_cost=step_cost,\n",
    ")\n",
    "featurecolors = {\n",
    "    '.': 'white',\n",
    "    'a': 'lightgreen',\n",
    "    'g': 'green',\n",
    "    'x': 'red',\n",
    "    'c': 'black',\n",
    "    'b': 'blue'\n",
    "}\n",
    "\n",
    "mdp2.plot(featurecolors=featurecolors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_erpi_params2 = dict(\n",
    "    transition_matrix=torch.tensor(mdp2.transition_matrix),\n",
    "    reward_matrix=torch.tensor(mdp2.reward_matrix),\n",
    "    discount_rate=torch.tensor(mdp2.discount_rate),\n",
    "    # the lower this is, the more optimal the policy\n",
    "    entropy_weight=torch.tensor([entropy_weight]),\n",
    "    n_planning_iters=10,\n",
    "    policy_prior=None,\n",
    "    initial_policy=None,\n",
    "    check_convergence=True,\n",
    "    force_nonzero_probabilities=True,\n",
    ")\n",
    "\n",
    "# Max Entropy IRL expert policy\n",
    "expert_erpi2 = entropy_regularized_policy_iteration(\n",
    "    **expert_erpi_params2\n",
    ")\n",
    "\n",
    "expert_policy2 = TabularPolicy.from_matrix(\n",
    "    states=mdp2.state_list,\n",
    "    actions=mdp2.action_list,\n",
    "    policy_matrix=expert_erpi2.policy.detach().numpy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurizer2(state, action, next_state):\n",
    "  feature_name = mdp2.location_features.get(next_state, 's')\n",
    "  if feature_name in 's':\n",
    "    return {}\n",
    "  return {feature_name: 1}\n",
    "\n",
    "\n",
    "def fixed_reward2(state, action, next_state):\n",
    "  return mdp2.step_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm2 = MaxLikelihoodIRL(mdp2, featurizer2, fixed_reward2, batch_size=batch_size, epochs=epochs, lr=lr, weight_decay=weight_decay, momentum=momentum, entropy_weight=entropy_weight)\n",
    "\n",
    "# compute policy from learned weights\n",
    "feature_reward_matrix2 = torch.einsum(\n",
    "    \"sanf,f->san\",\n",
    "    algorithm2.get_feature_matrix(),\n",
    "    learned_weights\n",
    ")\n",
    "\n",
    "reward_matrix2 = feature_reward_matrix2 + algorithm2.get_fixed_reward_matrix()\n",
    "terminal_index = mdp2.state_index.get(\n",
    "    frozendict({'x': -1, 'y': -1}))\n",
    "reward_matrix2[:, :, terminal_index] = 0\n",
    "\n",
    "my_erpi_params2 = dict(\n",
    "    transition_matrix=torch.tensor(mdp2.transition_matrix),\n",
    "    reward_matrix=reward_matrix2,\n",
    "    discount_rate=torch.tensor(mdp2.discount_rate),\n",
    "    # the lower this is, the more optimal the policy\n",
    "    entropy_weight=torch.tensor([entropy_weight]),\n",
    "    n_planning_iters=10,\n",
    "    policy_prior=None,\n",
    "    initial_policy=None,\n",
    "    check_convergence=True,\n",
    "    force_nonzero_probabilities=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset2 = FeaturesDataset(mdp2, trajectories, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.eval()\n",
    "\n",
    "temp_imitpolicy2 = {}\n",
    "# so pred doesn't have a grad_fn attached to it\n",
    "with torch.no_grad():\n",
    "  for s in mdp2.state_list:\n",
    "    state_feature = torch.from_numpy(\n",
    "        np.array([features_dataset2.getStateFeature(s)], dtype='f')).to(device)\n",
    "    # pred = model(state_feature)[0]\n",
    "    pred = optimized_model(state_feature)\n",
    "    probs = softmax_fn(pred)[0]\n",
    "    temp_imitpolicy2[s] = {}\n",
    "    # for each action in each state, grab from the \"pred\" result the probability of taking this action in this state and add that to the dictionary policy which is a dictionary with states as keys and a dictionary of \"action: prob of taking this action in this state\" as values\n",
    "    for a_i, prob in enumerate(probs):\n",
    "      temp_imitpolicy2[s][int_to_action[a_i]] = prob.item()\n",
    "\n",
    "# to turn policy into an actual policy object that we can call the MSDM functions on\n",
    "imitation_policy2 = TabularPolicy({s: DictDistribution(ap)\n",
    "                       for s, ap in temp_imitpolicy2.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve with matrices returned by gridworld object\n",
    "my_erpi2 = entropy_regularized_policy_iteration(\n",
    "    **my_erpi_params2\n",
    ")\n",
    "\n",
    "max_policy2 = TabularPolicy.from_matrix(\n",
    "    states=mdp2.state_list,\n",
    "    actions=mdp2.action_list,\n",
    "    policy_matrix=my_erpi2.policy.detach().numpy()\n",
    ")\n",
    "\n",
    "mdp2.plot(featurecolors=featurecolors).plot_policy(max_policy2)\n",
    "mdp2.plot().plot_state_map(max_policy2.evaluate_on(mdp2).V)\n",
    "\n",
    "mdp2.plot(featurecolors=featurecolors).plot_policy(expert_policy2)\n",
    "mdp2.plot().plot_state_map(expert_policy2.evaluate_on(mdp2).V)\n",
    "\n",
    "mdp2.plot(featurecolors=featurecolors).plot_policy(imitation_policy2)\n",
    "mdp2.plot().plot_state_map(imitation_policy2.evaluate_on(mdp2).V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max policy initial value: {max_policy2.evaluate_on(mdp2).initial_value}\")\n",
    "print(f\"Expert policy initial value: {expert_policy2.evaluate_on(mdp2).initial_value}\")\n",
    "print(f\"Imitation policy initial value: {imitation_policy2.evaluate_on(mdp2).initial_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a48d4cd50a2a48eca588c9887707ccbe5fc1fb45f33358dc08b670e55b95267a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('IRLIW': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
